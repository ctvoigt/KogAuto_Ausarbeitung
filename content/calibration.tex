\section{Kalibrierung der Brille}
Im folgenden Kapitel wird ein Verfahren für die Kalibrierung einer Durchsichtbrille (wie in Kapitel \ref{einleitung_hardware_subsec}  beschrieben), erläutert.
Hierbei werden folgende Fragen beantwortet: 
Wie wird die Wahrnehmung der 3D-Objekte mithilfe der Durchsichtbrille möglich?
Warum wird eine Kalibrierung der Brille benötigt? 
Wie ist die Brille zu kalibrieren, sodass eine Darstellung der Augmentierung korrekt möglich ist? 
Welche Parameter müssen individuell berechnet werden, welche können in der Kalibrierung ermitteln werden und welche können statisch angenommen werden?

\subsection{Stereoskopisches Sehen}
\label{sec:Stereoskopisches 3D}
\subsubsection{Allgemeine Information der räumlichen Wahrnehmung}
Zur räumlichen Wahrnehmung des Menschen tragen mehre Faktoren bei.
Die wichtigsten dieser Faktoren sind die relativer Größe eines Objektes zur anderen Objekten, die Verdeckung von Objekten und das stereoskopischen Sehen. 
Stereoskopisches Sehen vermittelt durch die beidäugige Betrachtung von Objekten und Gegenständen eine echte, messbare Tiefenwahrnehmung und räumliche Wirkung des Außenraums. 
Letzteres ist nur durch das Vorhandensein zweier Augen möglich.
Jedes Auge bekommt ein eigenes Bild und sendet dieses ins Gehirn.
Dort werden beide Bilder zu einer Raumwahrnehmung zusammengefasst.
Hierbei gibt es Entfernungsbereiche in denen das stereoskopische Sehen die wichtigste Rolle einnimmt.
Eine genauere Beschreibung ist im Abbildung \ref{fig:3D} \cite{stereo} zu finden.

\begin{figure}[h]
   \centering
   \includegraphics[width=0.45\textwidth]{3D-bild}
   \caption{stereoskopisches Sehen}
   \label{fig:3D}
\end{figure}

Da sich die Bilder des linken und rechten Auges sich von der Perspektive unterscheiden, kann das menschliches Gehirn aus diesen Unterschiede die Positionen, Formen und Größen der Objekte ermitteln.

Damit ein Bild dreidimensional wirkt, ist es hierbei ausreichend mit approximierten Werten zu arbeiten.
Man nimmt zwei sich in der Perspektive unterscheidende Bilder für die beiden Augen, deren Unterschiede sich an einem Standarmenschen orientieren. 
Hiermit erzeugt man eine 3D-Wahrehmung. 
Für manche Anwendungen (wie 3D-Filme, oder stereoskopische Bilder) ist dies ausreichend, da in diesem Anwendungsfall nur eine dreidimensionaler Eindruck hinterlassen werden soll. 
Für eine genaue Augmentierung reicht dies jedoch nicht aus, da mit diesem Verfahren nicht bei jedem Menschen, die gleiche Tiefenwahrnehmung erzeugt werden kann.
Folglich könnten alle Menschen diese Bilder als 3D interpretieren, aber die Position und sogar die Größe der gesichteten Objekte können sich stark unterscheiden. 
Aus dem Grund ist eine individuelle Kalibrierung vorzunehmen.


\subsubsection{Geometrische Beschreibung der Kalibrierung}
\label{sec:Geometrische Beschreibung der Kalibrierung}
Dieses Kapitel beschreibt wofür die Kalibrierung durchgeführt wird und welche Daten dafür benötigt werden.
Die gegebene Daten sind im Abbildung \ref{fig:geom} dargestellt. 

\begin{figure}[h]
   \centering
   \includegraphics[width=0.45\textwidth]{kalibr-geometrie}
   \caption{Geometrische Beschreibung der Kalibrierung}
   \label{fig:geom}
\end{figure}

Punkte $B_{l}$, $C_{l}$ bzw. $B_{r}$, $C_{r}$ sind die Endpunkte des linken bzw. rechten Bildschirms der Brille.
Die Punkte $A_{l}$ (grün) bzw. $A_{r}$ (rot) beschreiben die Positionen der Augen.
$wa_{l}$ und $wa_{r}$ sind die Sehwinkel des Benutzers durch welche die Bildschirme und dort dargestellte Objekte gesehen werden. 
Blaue Linien $M_{l}$ und $M_{r}$ sind Winkelhalbierenden.

Für die Anzeige der Bilder auf der Brillenbildschirme, werden Augen als Kameras interpretiert und damit werden benötigte Bilder erstellt.
Dementsprechend sind die Winkel $wa_{l}$ und $wa_{r}$ die Öffnungswinkel dieser Kameras.
Wie auf dieser Abbildung \ref{fig:geom} gut zu sehen ist, müssen die Augenpositionen nicht unbedingt in der Mitte des Bildschirms liegen.
In meisten Anwendungen, die Kameras simulieren, ist aber vorausgesetzt, dass die Kameras in der Mitte sind.
Das ist dasselbe, wie dass der Winkelhalbierende der Öffnungswinkel senkrecht zur Bildschirm ist.
Wir nehmen an, dass dafür benötigte Drehwinkel $g_{l}$ bzw. $g_{r}$ keine entscheidende Rolle in der Bildanzeige spielt.
Mit dieser Annahme rechnen wir die Winkel $g_{l}$ und  $g_{r}$ aus  um die virtuellen Kameras theoretisch zu drehen.
Das wird benötigt um passende Daten für die Benutzung anderer Anwendungen zu bekommen. Die konkrete Beschreibung der Realisierung dafür ist in Kapitel  \ref{sec:virtuelle Kameras} zu finden.

\subsection{Konzept}
Hierzu werden die Parameter\todo{Welche Parameter? Genau erklären, was wird benötigt, was kann gemesser, was berechnet und was bestimmt werden.} benötigt, mit denen die Bilder für ein solch akkurates Stereoscopic-3D benötigt werden.
(Für geometrische Beschreibung siehe Kapitel \ref{sec:Geometrische Beschreibung der Kalibrierung} )
Zu diesen Parametern zählen die Position der Augen ($A_{l}$; $A_{r}$), die Position der Bildschirme in den Brillengläsern ($B_{l}$, $C_{l}$ und $B_{r}$, $C_{r}$) und die Öffnungswinkel ($wa_{l}$; $wa_{r}$), mit denen ein einzelnes Individuum durch eine spezifische Brille sehen kann. (Siehe Abbildung \ref{fig:geom} )
Um die Position der Augen und die Öffnungswinkel, die sich von Benutzter zu Benutzter unterscheiden können, zu bestimmen wurde ein interaktives Kalibrierungsverfahren ausgewählt. Es ist wichtig, dass die Kalibrierung komfortabel und ohne externe\todo{ist der bildschirm keine externe gerätschaft - verhalten genauer erklären} Gerätschaften von statten geht, da es für jeden Benutzter nötig ist, die Brille individuell zu kalibrieren. Das Minimum an benötigten Punkten um die Position eines Auges zu bestimmen sind zwei Geraden, also zwei Punkte auf einer Sichtlinie je Geraden. Um die Daten präziser zu machen können mehrere Geraden verwendet werden, die dann einen genaueren Schnittpunkt ermöglichen. 

Der erste Ansatz war\todo{nur Präteritum. erster ansatz ist Ralf überhauptnicht klargeworden, eventuell einfach den ersten ansatz löschen} es einen Punkt im Auto zu verwenden, der in seiner Position bekannt ist und durch Bildverarbeitung erkannt werden kann und n Punkte auf der Brille. Hierbei hat sich jedoch herausgestellt, dass der Abstand der einzelnen Schnittlinien\todo{Was für schnittlinien?} in einem sehr kleinen Winkels von statten geht und somit fehleranfällig ist. Der zweite Ansatz verwendet den Bildschirm, der im Auto verbaut ist und die Brillengläser. Hiermit sind beidseitig die Punkte variabel und können somit so gewählt werden, dass sich Messfehler nicht so gravierend auswirken wie im vorherigen Ansatz.\todo{Echt? Das einzige, was entscheident ist, ist der Punkt, der auf dem Brillendisplay gewählt wird. Je weiter der Punkt am Rand liegt, desto größer ist der Winkel ind somit wird die Kalibrierung genauer.} Des Weiteren ist das Verfahren erweiterbar auf eine Onlinebewertung während der Kalibrierung, so dass über ein Algorithmus passende Punkte ausgewählt werden können, um die Kalibrierung zu optimieren.

Im Ablauf werden nun also Punkte auf dem Brillenglas angezeigt, die der Benutzer per Klick auf die Stelle auf dem Bildschirm markiert, so dass sie für ihn übereinander liegen. Somit konnte ein idealer Kompromiss zwischen Benutzerfreundlichkeit und der Akquisition von geeigneten Linienpaaren gefunden werden.

Nach dem Kalibrierungsprozess für jedes Auge können die individuellen Parameter fürs Benutzer berechnet werden.

\subsection{Implementierung}
\todo{Hier noch Text einfügen.}


\subsubsection{Kalibrierung}
\label{chap:punktepaare}
Um nun also Punktepaare für die Kalibrierung aufzeichnen zu können, ist die Grafikausgabe der Software nötig. Hierzu wurden zwei Anwendungen als ROS Nodes implementiert. Die Nodes werden mittels der Frameworks \ac{SDL}  \cite{sdl} und  \ac{Qt} \cite{qt} umgesetzt.
\subsubsection*{Anzeige auf der Brille}
Eine Vollbildanzeige mit fester Auflösung der nativen Auflösung der Brillenanzeige wird mittels SDL implementiert. Diese Anwendung erkennt automatisch die Brille als zweiten Bildschirm und wählt diesen für die Anzeige aus. Ansonsten ermöglicht die Anwendung es, einen Punkt an eine per ROS-Paket vorgegebene Position anzuzeigen.

\subsubsection*{Anzeige auf dem Bildschirm}
Um auf den im Auto integrierten Bildschirm eine Anzeige durchzuführen, wurde eine Oberfläche (siehe Abbildung \ref{fig:fensteranwendung}) mit Qt implementiert. Diese enthält die für die Bildverarbeitung nötigen Marker (siehe Kapitel \ref{ssection:alva}) und nimmt die Klickpositionen vom Nutzer entgegen. Die ermittelte Klickposition wird als ROS-Messages versendet.
\begin{figure}[h]
   \centering
   \includegraphics[width=0.45\textwidth]{bildschirm_anwendung}
   \caption{Oberfläche für den OnBoard Computer}
   \label{fig:fensteranwendung}
\end{figure}


\subsubsection{Koordinatentransformation}
Die Koordinatentransformationen sind nötig, um aus den Punktepaaren (Die über die Anwendungen aus Kapitel \ref{chap:punktepaare}), Koordinaten im 3D Weltkoordinatensystem zu erhalten. Nur aus Punkten in einem einheitlichen Koordinatensystem können später korrekte Geraden berechnet werden.
Die Eingaben für das Verfahren erhält die Anwendung aus ROS-Messages und published diese auch wieder auf gleichem Wege.

\subsubsection*{Weltkoordinatensystem}
\label{sec:koord}
Das Weltkoordinatensystem für die Transformation hat seinen Ursprung in der Kamera, gerichtet in Sichtrichtung mit der Konvention angelehnt an Kamerasysteme: x nach rechts, y nach unten, z nach vorne.

\subsubsection*{Transformation für Brillenpunkte}
Das ausgewählte Koordinatensystem macht es einfach die Transformation von einem Brillenpixel in das Weltkoordinatensystem abzubilden. 
Es handelt sich um eine statische Transformation, da die Kamera fest mit der Brille verschraubt ist. 
Die Herausforderung besteht darin, den virtuellen Bildschirm der Brille abzubilden. In den einzelnen Brillengläsern befindet sich Optik (Lines und ein Spiegel), mit denen dem Brillenträger ein großer Bildschirm Simuliert wird. 
Da es für uns jedoch nicht möglich war, festzustellen, ob für jeden Brillenträger die Anzeige gleich war, haben wir folgenden Verfahren angewendet: 
Wir haben den virtuellen Bildschirm, projiziert per Strahlensatz, auf einen weiteren virtuellen Bildschirm genau im Mittelpunkt der Brille festgelegt. 
Dieser Bildschirm ist unabhängig vom Benutzer, da zu Vermessung dieses Bildschirmes der Abstand zum virtuellen Bildschirm keine Rolle spielt.
Die Wahl des virtuellen Bildschirms genau in der Brillenmitte hat ebenfalls den Vorteil, dass die gewonnenen Daten später auch für die Berechnung des Öffnungswinkels verwendet werden können, da diese identisch mit den physikalischen Brillengläsern sind.
Die Position des Bildschirmes wird aus empirischen Ergebnissen gewonnen. 
Hierzu haben wir die Brille waagerecht zu einer Wand festgeschraubt und Punktepaare zu einem von Hand vermessenen Auge aufgenommen. 
Hierzu haben wir Punktekoordinaten auf dem Bildschirm zu konstanten Punkten auf der waagerecht zur Brille montierten Referenzwand (mit Punkten) vermessen. \todo{Einmal ist die Brille montiert und einmal die refernzwand. Klarstellen, dass durch diese Montierung nur die Entfernung, aber nicht eine Verschiebung oder Verdrehung möglich war.}
Die Brille wurde in die für den jeweiligen Probanden korrekte Position gebracht, so dass es genau für einen Öffnungswinkel die Referenzwand füllend im Bild sehen kann. 
Anschließend wird der Abstand zur Wand vermessen. 
Aus dem Abstand zur Wand und den vermessenen Punkten auf der Wand, konnte mit Hilfe der gemessenen Pixel auf der Brille, die Größe eines Pixel auf unserem virtuellen Bildschirm innerhalb der Brille errechnet werden. 
Der Abstand zweier Punkte auf dem Referenzbildschirm wird für jeden der zwei Probanden gleich vorgegeben. 
Die angeklickte Position der beiden Probanden ist ebenfalls annährend gleich, jedoch nicht der Abstand zur Wand, den die Probanden für ihr persönliches Sichtfeld wählen. 
Die abweichenden Sichtöffnungswinkel durch die Brille erklären sich, da die Augen je nach physiologischen Eigenschaften verschieden weit hinter der Brille befindlich sind. 
Hierdurch ist es möglich eine Fehlerrechnung über alle Daten durchzuführen, da wir 15 Punktepaare je Proband ermitteln. 
Hierdurch ergibt sich ein recht genaues, empirisch bestimmtes Ergebnis für die für das Verfahren verwendeten, virtuellen Brillenbildschirme: Siehe Tabelle \ref{tab:konstanteWerte}. \todo{Tabelle?}
Die Position der virtuellen Displays konnte bei diesem Vorgang relativ zur Kamera per Lineal vermessen werden.

 \begin{table}[h]
 \begin{tabular}{l|l}
  Eigenschaft & Wert \\
  \hline
  \hline
  Pixelgröße & $2.04*10^-3 cm/px$ \\
  Größe des Virtuellen Bildschirms & $2,61 cm$ x $1,57 cm$ \\
  Position des Bildschirms & +- $4$, $-3$, $-2$ cm \\
 \end{tabular}
 \label{tab:konstanteWerte}
 \caption{Empirisch ermittelte Werte}
 \end{table}
 \todo{Wie sieht das mit den weten 4,-3,-2 aus? die hatte ich irgendwann mal abends geschätzt, als ich die brille nicht zum messen dahatte. welceh werte sind in den code eingegangen, ich hoffe nicht die? könnte die ejmand mal raussuchen und aktualisiren?}
Durch diese Ausmessungen konnte die  Transformation eines Koordinatenpunktes u, v in Weltkoordinaten auf zwei einfache Transformationen vereinfacht werden

   \begin{enumerate}
      \item Statische Transformation: Translation vom Oberen linken virtuellen Bildschirmpixel zum Ursprung
      \item Dynamische Transformation: Translation um u,v Pixel nach obiger Vermessung umgerechnet in Meter
   \end{enumerate}


In der Implementierung wurde noch folgende Vereinfachung festgestellt: Da die virtuellen Bildschirme immer eine parallele Ebene zur x, y-Achsen-Ebene aufstellen, kann die Transformation durch eine einfache Addition durchgeführt werden.

\subsubsection*{Transformationen für Bildschirmpunkte}
\label{ssection:alva}
Die Transformation von einem Bildschirmpixel in das Weltkoordinatensystem stellt andere Herausforderungen. 
Der Bildschirm kann als eine physikalische Größe\todo{das ist keine physikalische größe} angesehen werden und vermessen werden. 
Es liegen konstante  \ac{DPI}-Werte vor, es kann also ebenfalls eine statische Transformation von einem Referenzpunkt auf dem Bildschirm zum angezeigten Punkt durchgeführt werden. 
Hierzu ist lediglich eine Transformation nötig, die die Eigenschaften des Bildschirms (Position und DPI) berücksichtigt. 
Der Referenzpunkt des Bildschirms ist in der Ebene des Bildschirms innerhalb unserer GUI festgelegt. Durch diese Festlegen bleibt unserer Anwendung unabhängig von Verschiebungen des Anwendungsfensters auf dem Bildschirm. 
Bei diesem Vorgehen besteht jedoch die größte Herausforderung darin, dass die Position des Referenzpunktes ermittelt werden muss. 
Hierzu wird die Kamera der Brille mit Bildverarbeitung eingesetzt. 
Dafür ist die Bibliothek ALVAR\todo{ist bereits als abkürzung mit webseite vorgestellt?}, die sich in ROS integrieren lässt, verwendet.
Drei ALVAR Marker werden mittels der Webcam erkannt und zur Entfernungsbestimmung vermittelt. 
Hiermit sind nun beide Transformationen auch für die Berechnung der Position vorhanden. 
Dieser Ansatz bietet den Nachteil, dass ALVAR eine gewisse Ungenauigkeit und eine gewisse Verzögerung mit sich bringt, wodurch die Benutzbarkeit erschwert wird. 
Ein längerfristiges Ziel sollte es also sein, die Integration des Headtrackingalgorithmuses auch in die Kalibrierung einzufügen.
Das konkrete Vorgehen beschreibt sich bezüglich der Transformation folgendermaßen:

   \begin{enumerate}
      \item Dynamische Transformation:  Translation  um u, v der auf der Fensterfläche angeklickt wurde auf den Referenzpunkt der Anwendung (konkret, dem Mittelpunkt des obigen linken Markers) 
      \item Dynamische Transformation: Translation um die von ALVAR bestimmte Entfernung
   \end{enumerate}

Um auf den Referenzpunkt zu gelangen, gibt es eine triviale Addition, um den statischen Offset der Anzeigelemente und der zentrierten ALVAR Position zu kompensieren. 
Um nun die Translation um einen u,v Pixel zu realisieren, wird eine Ebene durch die drei Marker gelegt und drauf die Verschiebung durchgeführt. 
Im Anschluss kann die Rücktransformation zum Koordinatenursprung geschehen.


\subsubsection{Bestimmung der Augenposition}
\label{sec:Augenposition}
Als Input für die Bestimmung der Augenposition dienen Punktpaare in Weltkoordinaten. Die Punktpaare bestehen jeweils aus einem Punkt auf dem Bildschirm der Brille und einem auf dem Monitor des Computers.

Im ersten Schritt erhalten wir eine Menge an Geraden indem wir durch jedes Punktpaar eine Gerade legen. Im zweiten Schritt berechnen wir aus dieser Menge an Geraden eine Schnittumgebung. Hierbei nehmen wir jede Gerade aus der Menge und berechnen für jedes andere verbleibende Element der Menge den entsprechenden Pseudoschnittpunkt. Als Pseudoschnittpunkt definieren wir den      Mittelpunkt des kleinsten Abstandes zwischen zwei Geraden.

Als Ergebnis dieser Schritte erhalten wir nun eine Menge an Pseudoschnittpunkten. Um hieraus nun die Position der beiden Augen zu berechnen nutzen wir das \emph{Fast and Robust Smallest Enclosing Balls} Verfahren von Gärtner \cite{gaertner}, mit dem man die kleinste Kugel um eine Menge an Punkten im dreidimensionalen Raum berechnen kann.

Wir haben das Verfahren zusätzlich um eine dynamische Optimierung erweitert, mit dem Ziel
Messfehler zu minimieren und eine höhere Genauigkeit zu erzielen.\\ 

Hierbei wird das Verfahren erst auf die Menge \emph{P} der Pseudoschnittpunkte angewandt, wir erhalten als Ergebnis die Kugel $K_i$ mit  dem Mittelpunkt $K_m$ und dem Radius $K_r$. Dann wird ein Punkt \emph{S} aus \emph{P} frei gewählt, mit der Einschränkung dass \emph{S} auf der Oberfläche der Kugel liegt, also \todo{Hier klarmachen, dass aus alle Punkte jeder einmal weggelassen wird und dann das beste ergebnis weiterverwendet wird.}

\begin{align}
| K_m - S | &= K_r
\end{align} Dann wird der Algorithmus von Gärtner erneut angewandt, auf die Menge \emph{P\textbackslash S}. Dies wird so lange ausgeführt, bis
der Radius $K_r$ der resultierenden Kugel \emph{K} unter einen gewissen Grenzwert fällt; der spezifische Grenzwert unserer Implementierung ist \emph{max\_eye\_width} = 0.02\todo{Bitte einheit dazuschreiben. Ich fände es besser, hierfür keine Variable zu nenen, sondern den Grenzwert zu beschrieben. Ist sonst unsere einzige Variable im ganzen Dokument.}.

Dieses Verfahren war sinnvoll für anfängliche Testdaten\todo{Nacherzählung, hier bitte den abschnitt sachlicher gestalten.}, welche sich aber im Laufe der Implementierung als fehlerbehaftet erwiesen. Die korrekten Daten unterschieden sich stark von den anfänglichen Annahmen, welche durch eine fehlerhafte Implementierung erzielt wurden. So lagen die errechneten Pseudoschnittpunkte viel näher beisammen als erwartet. Damit wurde der Mittelpunkt unseres errechneten Auges zunehmend ungenauer, da wir diesen mit dem Mittelpunkt
unserer Kugel \emph{K} gleichsetzten.

Um hier eine höhere Genauigkeit zu erzielen wird nun nach der Berechnung der Mittelpunkt $K_r$ ersetzt, $K_r$ = $K_{r_{mittelwert}}$. $K_{r_{mittelwert}}$ ist hierbei der Mittelwert der Menge $P_{opt}$ an Pseudoschnittpunkten, wobei $P_{opt}$ die Menge der Pseudoschnittpunkte ist die bei der letzten Iteration der dynamischen Optimierung in Betracht gezogen wurde.


\begin{align}
K_{r_{mittelwert}} &= \frac{\sum\limits_{i=1}^{count(P)} P_i}{count(p)} 
\end{align}

\subsection{Erzeugung der stereoskopischen Bilder mit einer RViz Camera}
Für die technische Realisierung des Stereoskopischen 3D werden alle erkannten Objekte in die virtuelle Welt auf korrespondierende Positionen übertragen. 
Zusätzlich dazu werden 2 Kameras in der virtuellen Welt definiert, die jeweils Bilder für das linke und rechte Brillendisplay liefern. Allerdings reicht eine einfache Augensimulation aufgrund der technischen Voraussetzungen nicht aus. 
Denn die realen Display ist sind nur ein kleiner integrierter Teil der Brillen und unterscheidet hinsichtlich der Brennweite und des Öffnungs- und Drehwinkels erheblich von den Standardeinstellungen einer virtuellen Kamera ab. (siehe Kapitel \ref{sec:Geometrische Beschreibung der Kalibrierung})
Aus diesem Grund sind an den virtuellen Kameras Konfigurationen und Änderungen notwendig, die nun im Weiteren erläutert werden soll. 


\subsubsection{Einstellung des Öffnungswinkels der virtuellen Kameras}
\label{sec:virtuelle Kameras}

Je nach anatomischen Aufbau des Gesichts hinsichtlich  der Nasenform und der Augenpositionen des menschlichen Nutzers, variiert der tatsächliche Öffnungswinkel, der aufgrund des Abstandes vom Auge zur Brille entsteht. Im Rahmen des Praktikums wurden mithilfe eines eigenständigen Versuches Augenpositionen relativ zur Kamera gemessen. Da die Lage der Brillendisplays zur Kamera als konstant angenommen wird, ist der Abstand zwischen Auge und Brillendisplays nun bestimmbar.

Da der Öffnungswinkel der virtuellen Kameras nicht direkt manipuliert werden kann, wird ein alternativer Weg verwendet. Dabei wurde die virtuelle Kamera mithilfe der intrinsischen Parameter verändert. Zum einen entspricht die Brennweite f der virtuellen Kamera dem Abstand der Augen zum Display, zum anderen bilden die Größen der Brillendisplays die Grundlage für die Berechnung der Auflösung der virtuellen Kameras. Konkrete Berechnung findet sich in der Projektionsmatrix in Formel \ref{table:instrisische Kameraparameter} wieder. $f_x$ und $f_y$ beschreiben die fokalen Längen in Pixel (d.h. Abstand*px/m), $c_x$ und $c_y$ beschreiben den "Principle Point" der Kamera, welche sich aus der Auflösung (1280x 760 für die Vuzix-Brille) ergibt. 

%

\begin{equation}
\begin{pmatrix}
f_x & 0& 0& c_x \\
0 & f_y & 0 & c_y\\ 
0 & 0&   1 & 0  
\end{pmatrix}
\label{table:instrisische Kameraparameter}
\end{equation}

Die Displaygrößen variieren je nach Abstand Auge-Display. Die gemessenen Werte befinden sich mit obengenannten Größen in Tabelle \ref{tab:konstanteWerte}.
%
Mit der kalibrierten Brennweite und dem Blickwinkel der virtuellen Kameras können nun Objekte in der richtigen Größen angezeigt werden, wie sie von den Augen wahrgenommen werden sollen. Allerdings weicht die wahrgenommene Position der Objekte von der realen Position noch stark ab. 

\subsubsection{Drehwinkeleinstellung}
3-D-Sehen wird erst ermöglicht, wenn für beide Augen ein entsprechender Drehwinkel (d.h. Drehung in der horizontalen Ebene) eigestellt werden. Dies wurde in Kapitel \ref{sec:Stereoskopisches 3D} erläutert.  Entsprechende Drehwinkeln für das linke Auge $g_{l}$ und für das rechte $g_{r}$ werden dabei jeweils für die korrespondierende, virtuelle Kamera verwendet.
Es ist dabei eine wichtige Annahme zu berücksichtigen. Im Verhältnis zum Brillendisplay liegen die Augen hinsichtlich der Höhe in der Mitte. Dies bedeutet dass eine Betrachtung der Drehung des Winkels in der vertikalen Ebene ebenfalls notwendig ist. Dies ist in realen Anwendungen nicht zu vernachlässigen und darf auch deshalb in späteren Entwicklungen nicht außer Acht gelassen werden.

Mithilfe obiger Konfigurationen lassen sie sowohl erkannte reale Objekte als auch virtuelle Objekte anzeigen.  Ein Beispiel dafür findet sich in Abbildung \ref{fig:Virtuelle Quadrate aus Prezi}

\begin{figure}[h]
   \centering
   \includegraphics[width=0.45\textwidth]{3d.png}
   \caption{Virtuelle Objekte für das linke und das rechte Auge}
   \label{fig:Virtuelle Quadrate aus Prezi}
\end{figure}

\subsection{Ausblick}
Um die Implementierung einsatztauglich zu machen, müsste für die Kalibrierung die dynamische Einblende der Punkteanzahl ergänzt werden. Während dem Kalibrieren kann die Güte der eingegebenen Punkte festgestellt werden, aus dieser kann abgeleitet werden, wie viele weitere Punkte noch nötig sind. Desweiteren wäre es möglich, dem Benutzer deutlicher zu signalisieren, mit welchem Auger er gerade kalibrieren muss.

Desweiteren wäre eine Plausibilisierung des Ergebnises angebracht, sodass Fehlkalibrierungen direkt erkannt werden können. Hierzu würden Schwellwerte ausreichen.

